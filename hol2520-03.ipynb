{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260bb144-6a26-486d-b47a-ed1ec9c269e1",
   "metadata": {},
   "source": [
    "![Top <](./images/watsonxdata.png \"watsonxdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84637db2-2224-4866-962c-7b7780dd266a",
   "metadata": {},
   "source": [
    "# Lab 3: Filtered Queries\n",
    "\n",
    "In the previous lab searches were only performed on the vector field. In this lab the nearest neighbor search on the vector field is combined with additional search conditions on scalar fields.\n",
    "\n",
    "The first steps for creating and loading a database are the same as in lab 1 and 2. If you just have executed lab 1 you can quickly execute the following steps until you reach the section **Filtered Querying of Milvus**. \n",
    "\n",
    "The first step is to make sure that the Milvus extensions are loaded into the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb61b9c-2cee-4097-9825-561bff73af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymilvus==2.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c6e6d-b973-42d5-b883-1733700e6d02",
   "metadata": {},
   "source": [
    "## Local Connection\n",
    "\n",
    "A local connection assumes that you are running your Jupyter notebook inside the same server that is running watsonx.data and the Milvus server. The connection user is the default watsonx.data userid (ibmlhadmin). You need to generate the certificate that will be used by the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69e4a4-7d5d-47bc-999a-bd4a804cdc0f",
   "metadata": {},
   "source": [
    "### Generate the Connection Certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fcd41-b112-4728-850f-d2d0adbdb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f /tmp/presto.cert\n",
    "!echo QUIT | openssl s_client -showcerts -connect localhost:8443 | awk '/-----BEGIN CERTIFICATE-----/ {p=1}; p; /-----END CERTIFICATE-----/ {p=0}' > /tmp/presto.crt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d7823-aa5b-40f2-9e61-0a4127f59f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = %system echo QUIT | openssl s_client -showcerts -connect watsonxdata:8443 | \\\n",
    "        awk '/-----BEGIN CERTIFICATE-----/ {p=1}; p; /-----END CERTIFICATE-----/ {p=0}' > /tmp/presto.crt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e05e1-98a0-463e-9348-57660e144729",
   "metadata": {},
   "source": [
    "### Local Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313a49a-fa0c-4d49-91fe-48a6c77f0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "host            = 'watsonxdata'\n",
    "port            = 19530\n",
    "apiuser         = 'xxxxxxxxxx'\n",
    "apikey          = 'xxxxxxxx'\n",
    "server_pem_path = '/tmp/presto.crt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7519ee-3ff9-4952-a1b7-8280857547be",
   "metadata": {},
   "source": [
    "## Milvus Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca636f6-3549-43d4-aa53-55406453e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import(\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    ")\n",
    "\n",
    "connections.connect(alias='default',\n",
    "                   host=host,\n",
    "                   port=port,\n",
    "                   user=apiuser,\n",
    "                   password=apikey,\n",
    "                   server_pem_path=server_pem_path,\n",
    "                   server_name='watsonxdata',\n",
    "                   secure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88891718-6a17-48a4-818a-4a0ec22033fd",
   "metadata": {},
   "source": [
    "### Check Connection Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c68194-8f9f-4334-8361-1864f6601286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nList connections:\")\n",
    "print(connections.list_connections())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f277f-e5bf-475a-b422-eefe1fe604ed",
   "metadata": {},
   "source": [
    "## Create a Collection in Milvus\n",
    "This code will drop the wiki_articles collection if it exists, and then recreate it. This script should return the following text.\n",
    "```\n",
    "Status(code=0, message=)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0dfcc0-03db-4a40-8be8-5e9c775d0aa3",
   "metadata": {},
   "source": [
    "#### Make various unitilty commands available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6aa9c-355e-4f9b-90a8-98f0de2f1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3feb49-1e5e-411f-81f9-d4090beaadf4",
   "metadata": {},
   "source": [
    "#### Clean up previous collection if one already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557171e-6467-4a89-898b-b6d1a4b1fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.drop_collection(\"wiki_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff224431-7c36-4153-a295-71b9193dbfef",
   "metadata": {},
   "source": [
    "#### Create a sample collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f0198-bc1a-483c-83fc-e7e114b1b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True), # Primary key\n",
    "    FieldSchema(name=\"article_text\", dtype=DataType.VARCHAR, max_length=2500,),\n",
    "    FieldSchema(name=\"article_title\", dtype=DataType.VARCHAR, max_length=200,),\n",
    "    FieldSchema(name=\"article_subtopic\", dtype=DataType.VARCHAR, max_length=10,),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, \"wikipedia article collection schema\")\n",
    "\n",
    "wiki_collection = Collection(\"wiki_articles\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2e3fb-d8b6-4578-aa2a-20aa1be70ffa",
   "metadata": {},
   "source": [
    "#### Create an index for this collection\n",
    "\n",
    "- metric_type specifies the distance metric used in the vector space. L2 is the Euclidian distance.\n",
    "- index_type specifies the type of vector index to use. IVF means inverted file index which means clusting the the vector space and representing each cluster by its centroid. FLAT means that vectors are stored directly without any compression or quantization meaning that precise distance calculations are possible\n",
    "- params specifies several parameters relevant for our index. For instance nlist defines the number clusters to use for the inverted file index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c899e5f-3847-4ce6-82ec-3f8b13fa1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "        'metric_type':'L2',\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\":2048}\n",
    "}\n",
    "\n",
    "wiki_collection.create_index(field_name=\"vector\", index_params=index_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21e7b8-6212-465b-9ce2-dd1d50a8ebee",
   "metadata": {},
   "source": [
    "#### Double Check that the Schema Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ae789-f4d8-4a14-a119-15b7d9cccc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c19f27-ab7d-4d31-94e0-b340423076ea",
   "metadata": {},
   "source": [
    "## Get data from Wikipedia for loading into our collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddeba1-e82d-45c4-be19-85448d73ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# search\n",
    "search_results = wikipedia.search(\"Climate\")\n",
    "\n",
    "articles = []\n",
    "for i in range (0,len(search_results)):\n",
    "    try:\n",
    "        summary = wikipedia.summary(search_results[i],auto_suggest=False)\n",
    "    except Exception as err:\n",
    "        print(f\"Skipped article '{search_results[i]}' skipped because of ambiguity.\")\n",
    "        continue\n",
    "    try:\n",
    "        page = wikipedia.page(search_results[i],auto_suggest=False).content\n",
    "    except Exception as err:\n",
    "        print(f\"Skipped article '{search_results[i]}' skipped because of ambiguity.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    articles.append({\n",
    "        \"title\"   : search_results[i],\n",
    "        \"summary\" : summary,\n",
    "        \"page\"    : page\n",
    "    })\n",
    "\n",
    "#print(display_articles)\n",
    "\n",
    "df = pd.DataFrame.from_dict(articles)\n",
    "df.style.set_properties(**{'text-align': 'left'})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b35b0-55b7-4a74-b5f1-8018e89b81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2c1f3-b919-45dc-9a98-ccebe7480b6b",
   "metadata": {},
   "source": [
    "## Split Articles into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8d8fd-844e-4315-b82d-c6bb3037340b",
   "metadata": {},
   "source": [
    "### Define function for splitting article into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fabfe-715c-4e81-a245-9865e13f69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    #print('text:',text)\n",
    "    #print('words:',words)\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64ad40-edb9-413f-a90e-1802666a9f88",
   "metadata": {},
   "source": [
    "### Create list of chunks for all articles and create analog list for additional metadata correspong to the chunk (title, subtopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9df370-2183-4b89-97a3-01bd675feb4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunk_size=255\n",
    "passages=[]\n",
    "passages_titles=[]\n",
    "passages_subtopic=[]\n",
    "\n",
    "for a in articles:\n",
    "    print('title',a['title'])\n",
    "    if a['title'] == \"Climate\":\n",
    "        subtopic=\"false\"\n",
    "    else:\n",
    "        subtopic=\"true\"\n",
    "\n",
    "    p = a['page']\n",
    "    cl = split_into_chunks(p,chunk_size)\n",
    "\n",
    "    print(\"number of chunks=\",len(cl))\n",
    "    for c in cl:\n",
    "        passages.append(c)\n",
    "        passages_titles.append(a['title'])\n",
    "        passages_subtopic.append(subtopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd36679-b940-488c-9369-20d543a58626",
   "metadata": {},
   "source": [
    "### Create the embeddings for the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10145813-aa87-4117-9492-d98200b06585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # 384 dim\n",
    "passages_embeddings = model.encode(passages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd52d3-791b-473c-972d-742cf129d7de",
   "metadata": {},
   "source": [
    "### Insert all data into the collection created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cad62-a4e2-4b51-b946-6d63c8496e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_collection = Collection(\"wiki_articles\") \n",
    "data = [\n",
    "    passages,\n",
    "    passages_titles,\n",
    "    passages_subtopic,\n",
    "    passages_embeddings\n",
    "]\n",
    "out = basic_collection.insert(data)\n",
    "basic_collection.flush()  # Ensures data persistence\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db6a6c-fb57-472a-ba53-4873d48b0630",
   "metadata": {},
   "source": [
    "## Filtered Querying of Milvus \n",
    "\n",
    "The following code shows how you can query by specifying a scaler condition in addition to the vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5754b93-b7b4-4c11-a569-b4d0e570ecf7",
   "metadata": {},
   "source": [
    "### Load the Collection into memory and check that the Collection has been Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b9c80-961d-4736-b8f0-8f584226694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_collection = Collection(\"wiki_articles\") \n",
    "basic_collection.load()\n",
    "basic_collection.num_entities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92d642-c899-4187-9df8-1a5d780c8cb3",
   "metadata": {},
   "source": [
    "## Query Milvus & Prompt LLM\n",
    "After gathering the data from Wikipedia and then vectorizing it and inserting into Milvus, we are now ready to perform queries against the vector database. We will use the `sentence-transformers/all-MiniLM-L6-v2` model to generate the query vector and then use Milvus to find the most similar vectors in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212fbf2-ea6e-4993-b045-9aba27e1b843",
   "metadata": {},
   "source": [
    "### Create a Query Function\n",
    "The following function will be used to query the Milvus database with filtering. The main difference to lab 2 is that now additional parameter search_expression is passed to query_milvus which specifies the additional scalar filter expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5905470-9db1-4571-9db9-7c574775fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import(\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    ")\n",
    "\n",
    "def query_milvus(query, search_expression, num_results=5):\n",
    "    \n",
    "    # Vectorize query\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # 384 dim\n",
    "    query_embeddings = model.encode([query])\n",
    "\n",
    "    # Search\n",
    "    search_params = {\n",
    "        \"metric_type\": \"L2\", \n",
    "        \"params\": {\"nprobe\": 5}\n",
    "    }\n",
    "    results = basic_collection.search(\n",
    "        data=query_embeddings, \n",
    "        anns_field=\"vector\", \n",
    "        param=search_params,\n",
    "        limit=num_results,\n",
    "        expr=search_expression, \n",
    "        output_fields=['article_text','article_subtopic'],\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cb9bb-ebba-427d-accf-2eb4a08425cc",
   "metadata": {},
   "source": [
    "### Prompt LLM with Query Results\n",
    "Consider how climate change may relate to other industries and processes related to your business. Select one of the questions below to feed into Milvus query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53ed21-e3d6-41a6-a036-a92e9cb8ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"What can my company do to help fight climate change?\"\n",
    "#question_text = \"How do businesses negatively effect climate change?\"\n",
    "#question_text = \"What can a businesses do to have a positive effect on climate change?\"\n",
    "#question_text = \"How can a business reduce their carbon footprint?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72d637-d4c8-4cc3-a75d-14a4d635e4c9",
   "metadata": {},
   "source": [
    "### Search a Question in Milvus\n",
    "\n",
    "We want to use the above question_text to perform a approximate nearest neighbor search in Milvus. But in addition we want to filter on the scalar column article_subtopic. We want only results where article_subtopic is \"false\". We search for the most relevant chunks with these two conditions. The top 3 related chunks are retrieved below and can be used for a large language prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfe039-4763-4278-b843-d408c10a4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 3\n",
    "filter='article_subtopic == \"false\"'\n",
    "results = query_milvus(question_text, filter, num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a12b5-6b44-4e68-9143-184677d4e928",
   "metadata": {},
   "source": [
    "## Display result\n",
    "\n",
    "The documents that best match the question are now displayed in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7783e1-9602-4fc1-b669-a44e18c0fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "display_articles = []\n",
    "relevant_chunks  = []\n",
    "for i in range(num_results):\n",
    "    display_articles.append({\n",
    "        \"ID\"      : results[0].ids[i],\n",
    "        \"Distance\": results[0].distances[i],\n",
    "        \"Subtopic\": results[0][i].entity.get('article_subtopic'),\n",
    "        # \"Article\" : re.sub(r\"^.*?\\. (.*$)\",r\"\\1\",results[0][i].entity.get('article_text'))\n",
    "        \"Article\" : re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text'))        \n",
    "    })\n",
    "    relevant_chunks.append(re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text')))\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles).sort_values(\"Distance\",ascending=False)\n",
    "df.style.set_properties(**{'text-align': 'left'}).set_caption(question_text).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce86e4-878b-47cf-ad04-dd034c3e2bfc",
   "metadata": {},
   "source": [
    "### Search a Question in Milvus again\n",
    "\n",
    "We want to repeat the above query but now we want chunks where article_subtopic is \"true\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d85d2-29a9-4d7a-95e9-c61a689c8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 3\n",
    "filter='article_subtopic == \"true\"'\n",
    "results = query_milvus(question_text, filter, num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22c476-5778-423c-b7d4-82ecb583ba4b",
   "metadata": {},
   "source": [
    "## Display result\n",
    "\n",
    "The documents that best match the question are now displayed in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834fe3a-bde9-48aa-bbc2-276eafb34f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "display_articles = []\n",
    "relevant_chunks  = []\n",
    "for i in range(num_results):\n",
    "    display_articles.append({\n",
    "        \"ID\"      : results[0].ids[i],\n",
    "        \"Distance\": results[0].distances[i],\n",
    "        \"Subtopic\": results[0][i].entity.get('article_subtopic'),\n",
    "        # \"Article\" : re.sub(r\"^.*?\\. (.*$)\",r\"\\1\",results[0][i].entity.get('article_text'))\n",
    "        \"Article\" : re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text'))        \n",
    "    })\n",
    "    relevant_chunks.append(re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text')))\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles).sort_values(\"Distance\",ascending=False)\n",
    "df.style.set_properties(**{'text-align': 'left'}).set_caption(question_text).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a62d83-4cf1-4f11-bdfd-60dc3ad5b06f",
   "metadata": {},
   "source": [
    "The distance of the chunks from the query_text are also very different depending on the scalar condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4fff7-8ff1-43dc-a25a-9b03e463e7a3",
   "metadata": {},
   "source": [
    "### Let us check the query result without an additional scalar condition\n",
    "\n",
    "We want to repeat the above query but now we want chunks without restricting on an additional scaler condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e77ec-907d-47da-9a29-10a02b22a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 3\n",
    "filter=None\n",
    "results = query_milvus(question_text, filter, num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c5f0d-d973-4bac-af12-50708f353ea6",
   "metadata": {},
   "source": [
    "## Display result\n",
    "\n",
    "The documents that best match the question are now displayed in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25866a30-33bd-4dd3-afee-259e59ca5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "display_articles = []\n",
    "relevant_chunks  = []\n",
    "for i in range(num_results):\n",
    "    display_articles.append({\n",
    "        \"ID\"      : results[0].ids[i],\n",
    "        \"Distance\": results[0].distances[i],\n",
    "        \"Subtopic\": results[0][i].entity.get('article_subtopic'),\n",
    "        # \"Article\" : re.sub(r\"^.*?\\. (.*$)\",r\"\\1\",results[0][i].entity.get('article_text'))\n",
    "        \"Article\" : re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text'))        \n",
    "    })\n",
    "    relevant_chunks.append(re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text')))\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles).sort_values(\"Distance\",ascending=False)\n",
    "df.style.set_properties(**{'text-align': 'left'}).set_caption(question_text).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413cf1b-f8f3-47b8-91ff-906a5a42c3ae",
   "metadata": {},
   "source": [
    "#### Credits: IBM 2025, Wilfried Hoge [hoge@de.ibm.com] and Andreas Weininger [andreas.weininger@de.ibm.com] based on a notebook by George Baklarz [baklarz@ca.ibm.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
